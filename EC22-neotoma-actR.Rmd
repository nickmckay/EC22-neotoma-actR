---
title: "Database interoperability, uncertainty quantification and reproducible workflows in the paleogeosciences"
author:
- name: MyName
  email: authorone@example.com
  affiliation: Institute One
  correspondence: no
  url: http://example.com/personone
- name: MyName
  email: authortwo@example.com
  affiliation: Institute One; Insitute Two
  correspondence: yes
  url: http://example.com/persontwo
date: "2022/10/23"
output:
  html_document:
    number_sections: yes
    toc: yes
  pdf_document:
    toc: yes
institute:
- name: Institute One
  address: 5555 Example St
  city: City
  region: Province
  country: Country
- name: Institute Two
  address: 4444 Demonstration Ave
  city: town
  region: State
  country: Country
awards:
- ShortAwardName:
    sponsor: Example.com Award Foundation
    identifier: 25002020
    name: Collaborative Award to something or other.
    url: https://example.com/awards/25002020
- NextShortAwardName:
    sponsor: Example.com Award Foundation
    identifier: 25002021
    name: Collaborative Award to something or other.
    url: https://example.com/awards/25002021

bibliography: yourbibtex.bib
keywords:
- bullet
- points
- for
- each
- keyword
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Please use the `pacman` package to load your packages:
 #install.packages("pacman",repos = "https://cran.rstudio.com/")
 pacman::p_load("dplyr","magrittr","tidyr")
 pacman::p_load_gh("nickmckay/lipdR", "nickmckay/geoChronR", "NeotomaDB/neotoma2", "LinkedEarth/actR")
```

# Abstract

The paleogeosciences are becoming more and more interdisciplinary, and studies increasingly rely on large collections of data derived from multiple data repositories. Integrating diverse datasets from multiple sources into complex workflows increases the challenge of creating reproducible and open science, as data formats and tools are often noninteroperable, requiring manual manipulation of data into standardized formats, resulting in a disconnect in data provenance and confounding reproducibility. Here we present a notebook that demonstrates how the Linked PaleoData (LiPD) framework is used as an interchange format to allow data from multiple data sources to be integrated in a complex workflow using emerging packages in R for geochronological uncertainty quantification and abrupt change detection. Specifically, in this notebook, we use the neotoma2 and lipdR packages to access paleoecological data from the Neotoma Database, and paleoclimate data from compilations hosted on Lipdverse. Age uncertainties for datasets from both sources are then quantified using the geoChronR package, and those data, and their associated age uncertainties, are then investigated for abrupt changes using the actR package, with accompanying visualizations. The result is an integrated, reproducible workflow in R that demonstrates how this complex series of multisource data integration, analysis and visualization can be integrated into an efficient, open scientific narrative.

# Purpose

This notebook showcases a workflow that uses emerging tools to efficiently access a pollen dataset from Neotoma (ref), create an age model using Bacon (ref), store the age ensemble and create visualizations using `geoChronR`, before finally conducting age-uncertain change detection using the Abrupt Change Toolkit in R (actR) (ref).



The RMarkdown file should render successfully using the commandline argument `Rscript -e 'rmarkdown::render("yourfile.Rmd")'`.  If it uses some other method for rendering, please ensure that the reviewers are aware of this.

# Technical contributions

This notebook highlights recent advances in multiple R packages

* The new `neotoma2` package (cite)
* An overhauled `neotoma2lipd()` function in the `lipdR` package based on `neotoma2` that converts a `neotoma2` site object into a `lipdR` lipd object 
* Using lipd objects and `geoChronR` to create age models and store ensemble data
* Conduct robust, age-uncertain, abrupt change analysis on neotoma data using `actR`

# Methodology

`neotoma2` leverages Neotoma's xxx API, geojson and S4 structures. `lipdR::neotoma2lipd()` converts `neotoma2` site objects into `lipdR` lipd objects.

Most cells in this notebook are markdown cells. They describe the expected content of the notebook, steps for formatting and sumitting notebooks, and relevant technologies. The proposed structure and procedures are based on community input and literature (see References).


Markdown headings are automatically numbered and organized into a Table of Contents (ToC) using a table of contents, implemented in the `yaml` header:

```yaml
toc: true
```

# Results

Describe and comment on the most important results. Include images and URLs as necessary.

This template presents the general structure of notebooks expected by EarthCube, along with recommendations for additional resources to be created to ensure that the notebook is easy to re-use.

# Funding

Include references to awards that supported this research. Add as many award references as you need, using the YAML template included at the head of this document.


# Keywords

Include up to 5 keywords, within the YAML header above, using the format:

```yaml
keywords:
    - template
    - EarthCube
    - some subject
    - science
    - cool stuff
```

# Citation

Include the recommended citation for the document.  You may choose to use Zenodo, or another service (such as figShare or your University services) to obtain a DOI for your code repository.  If you are using GitHub, consider adding a [GitHub Citation file](https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/about-citation-files).  Ensure that the title in the suggested citation, and the authors match those in your YAML header.

For example:

* EarthCube Office, 2021. EarthCube Notebook Template. Accessed 2/1/2021 at https://github.com/earthcube/NotebookTemplates

# Acknowledgements

Include any relevant acknowledgements, apart from funding (which was in section 5).


The template is licensed under a [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/).

# Setup

You will need to download the Rmd, HTML and bibtex file from the earthcube repository, store it in a single directory and then set up a project (see [this link](https://swcarpentry.github.io/r-novice-gapminder/02-project-intro/) for further discussion of this.)

## Packages 
Install all the required packages in the r setup section at the head of this document. As noted previously, use pacman to do the installation.

## Data processing and analysis
The core of the notebook is here. Split this section into subsections as required, and explain processing and analysis steps.


```{r}
L <- get_sites(sitename = "Bambili 2") %>%
  get_downloads() %>%
  neotoma2lipd()
```


```{r}
L <- runBacon(L,
              lab.id.var = 'neotomaChronConrolId',
              age.14c.var = NULL,
              age.14c.uncertainty.var = NULL,
              age.var = 'age',
              age.uncertainty.var = 'ageUnc',
              depth.var = 'depth',
              reservoir.age.14c.var = NULL,
              reservoir.age.14c.uncertainty.var = NULL,
              rejected.ages.var = NULL,
              accept.suggestions = TRUE)

```

```{r}
L <- mapAgeEnsembleToPaleoData(L)

ageEnsemble <- selectData(L,"ageEnsemble")

```




```{r}
TRSH <- as.lipdTsTibble(L) %>%
  dplyr::filter(paleoData_ecologicalgroup == "TRSH") %>%
  tabulateTs(time.var = "age") %>% 
  rowwise() %>%
  mutate(total = sum(c_across(-age),na.rm = TRUE))
```


```{r}
total <- as.lipdTsTibble(L) %>%
  dplyr::filter(paleoData_ecologicalgroup %in% c("TRSH","UPHE","VACR","SUCC","MANG")) %>%
  tabulateTs(time.var = "age") %>%
  rowwise() %>%
  mutate(total = sum(c_across(-age),na.rm = TRUE))
```


```{r}
treeShrubPercent <- TRSH$total/total$total
plotTimeseriesEnsRibbons(X = ageEnsemble, Y =  treeShrubPercent)

```

Selected best practices for organizing and formatting your notebooks are included below.

## The 10 rules
These recommendations are based on (@rule_ten_2019):

1. Tell a story for an audience: interleave explanatory text with code and results to create a computational narrative.

2. Document the process, not just the results: so that others, and yourself later, will understand your reasoning and choices.

3. Use cell divisions to make steps clear: have each piece of R perform one meaningful and documented step, avoid long pieces of R.

4. Modularize code: use and document functions for repeated operations to make the code more readable and save space.

5. Record dependencies: use (for example) Conda package manager, with environment.yml (Conda). Please indicate package versions explicitly. Ideally, include a listing of dependencies at the bottom of the notebook. Make sure you test your notebook in an environment created from these dependencies, so that you don't add undocumented dependencies (we discuss this more in using Binder.

6. Use version control such as git.

7. Build a pipeline: place key variable declarations at the top. Test with different parameters, clean up, and run all R code  to prepare for potential non-interactive execution.

8. Share and explain your data: if the original data are too big, include a sample. Make sure that the data is accessible. Include data description, and any processing done beforehand. Ideally, point to datasets that have been permanently managed and identified by DOIs.

9. Design your notebooks to be read, run, and explored:  
    9.1. Read: store it in a public code repository with a clear README file and a liberal open source license. Generate HTML/PDF versions of the final notebook.  
    9.2. Run: use Binder to run it in the cloud.  
    9.3. Explore: use interactive widgets (e.g., Shiny) 
    
10. Advocate for open research: ask your colleagues or friends to try to run your notebooks.


# References