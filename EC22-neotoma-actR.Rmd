---
title: "Database interoperability, uncertainty quantification and reproducible workflows in the paleogeosciences"
author:
- name: Nicholas McKay
  email: nick@nau.edu
  affiliation: Northern Arizona University
  correspondence: yes
  url: http://nau.edu/mckay
- name: Julien Emile-Geay
  email: julieneg@usc.edu
  affiliation: University of Southern California
  correspondence: no
- name: Deborah Khider
  email: Khider@usc.edu
  affiliation: University of Southern California
  correspondence: no
date: "2022/10/23"
output:
  html_document:
    number_sections: yes
    toc: yes
  pdf_document:
    toc: yes
institute:
- name: Northern Arizona University
  city: Flagstaff
  region: Arizona
  country: USA
- name: University of Southern California
  city: Los Angeles
  region: California
  country: USA
awards:
- ShortAwardName:
    sponsor: Belmont Forum's Science-driven e-Infrastructure Innovation (SEI) initiative
    identifier: NSF-ICER-1929460
    name: ACCEDE
    url: https://www.nsf.gov/awardsearch/showAward?AWD_ID=1929460&HistoricalAwards=false

bibliography: book.bib
keywords:
- paleoclimate
- paleoecology
- abrupt change
- R
- LiPD
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
 #install.packages("pacman",repos = "https://cran.rstudio.com/")
 pacman::p_load("dplyr","magrittr","tidyr")
 pacman::p_load_gh("nickmckay/lipdR@neotoma2lipd", "nickmckay/geoChronR", "NeotomaDB/neotoma2", "LinkedEarth/actR")
```

# Abstract

The paleogeosciences are becoming more and more interdisciplinary, and studies increasingly rely on large collections of data derived from multiple data repositories. Integrating diverse datasets from multiple sources into complex workflows increases the challenge of creating reproducible and open science, as data formats and tools are often noninteroperable, requiring manual manipulation of data into standardized formats, resulting in a disconnect in data provenance and confounding reproducibility. Here we present a notebook that demonstrates how the Linked PaleoData (LiPD) framework is used as an interchange format to allow data from multiple data sources to be integrated in a complex workflow using emerging packages in R for geochronological uncertainty quantification and abrupt change detection. Specifically, in this notebook, we use the neotoma2 and lipdR packages to access paleoecological data from the Neotoma Database, and paleoclimate data from compilations hosted on Lipdverse. Age uncertainties for datasets from both sources are then quantified using the geoChronR package, and those data, and their associated age uncertainties, are then investigated for abrupt changes using the actR package, with accompanying visualizations. The result is an integrated, reproducible workflow in R that demonstrates how this complex series of multisource data integration, analysis and visualization can be integrated into an efficient, open scientific narrative.

# Purpose

This notebook showcases a workflow that uses emerging tools to efficiently access a pollen dataset from Neotoma (ref), create an age model using Bacon (ref), store the age ensemble and create visualizations using `geoChronR`, before finally conducting age-uncertain change detection using the Abrupt Change Toolkit in R (actR) (ref).

# Technical contributions

This notebook highlights recent advances in multiple R packages

* The new `neotoma2` package (cite)
* An overhauled `neotoma2lipd()` function in the `lipdR` package
* Using lipd objects and `geoChronR` to create age models and store ensemble data
* Conduct robust, age-uncertain, abrupt change analysis on neotoma data using `actR`

# Methodology

`neotoma2` leverages Neotoma's xxx API, geojson and S4 structures. `lipdR::neotoma2lipd()` converts `neotoma2` site objects into `lipdR` lipd objects.


```yaml
toc: true
```

# Results

This notebook demonstrates how the Linked PaleoData (LiPD) framework can be used as an interchange format to allow data from multiple data sources to be integrated in a complex workflow using emerging packages in R for geochronological uncertainty quantification and abrupt change detection. Efficient workflows that support data from multiple sources are an important component of reproducible science.

# Funding

This project is funded as part of the Belmont Forum's Science-driven e-Infrastructure Innovation (SEI) initiative. In the US, this is NSF award ICER-1929460.


# Citation

Include the recommended citation for the document.  You may choose to use Zenodo, or another service (such as figShare or your University services) to obtain a DOI for your code repository.  If you are using GitHub, consider adding a [GitHub Citation file](https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/about-citation-files).  Ensure that the title in the suggested citation, and the authors match those in your YAML header.

For example:

* EarthCube Office, 2021. EarthCube Notebook Template. Accessed 2/1/2021 at https://github.com/earthcube/NotebookTemplates

# Acknowledgements

Thank you to Socorro Dominguez Vidana and Simon Goring for their work on the `neotoma2` and to the scientists who generated and shared the Bambili pollen data and the MD03-2707 paleotemperature data. 

# Setup

You will need to download the Rmd, HTML and bibtex file from the earthcube repository, store it in a single directory and then set up a project (see [this link](https://swcarpentry.github.io/r-novice-gapminder/02-project-intro/) for further discussion of this.) Then you can either knit the markdown file, or run the cells interactively. 

## Packages 
Install all the required packages in the R setup section at the head of this document. As noted previously, use pacman to do the installation. Alternatively, use the `install.R` script in the repository, or run on mybinder, if that ever works.

## Data processing and analysis

The paleogeosciences are becoming more and more interdisciplinary, and studies increasingly rely on large collections of data derived from multiple data repositories. Integrating diverse datasets from multiple sources into complex workflows increases the challenge of creating reproducible and open science, as data formats and tools are often noninteroperable, requiring manual manipulation of data into standardized formats, resulting in a disconnect in data provenance and confounding reproducibility. Here, we illustrate a use case where a scientist wants to conduct age-uncertain abrupt-change analysis on two datasets from Africa - a pollen record from Lake Bambili, and a temperature reconstruction from the Gulf of Guinea. 


### Lake Bambili 

Lake Bambili is a .....
Recently, Lezine et al. published a study...

```{r}
L <- neotoma2::get_sites(sitename = "Bambili 2") %>%
  neotoma2::get_downloads() %>%
  lipdR::neotoma2lipd()
```


```{r}
mapLipd(L,map.type = "stamen",extend.range = 5)
```


```{r}
L <- runBacon(L,
              lab.id.var = 'neotomaChronConrolId',
              age.14c.var = NULL,
              age.14c.uncertainty.var = NULL,
              age.var = 'age',
              age.uncertainty.var = 'ageUnc',
              depth.var = 'depth',
              reservoir.age.14c.var = NULL,
              reservoir.age.14c.uncertainty.var = NULL,
              rejected.ages.var = NULL,
              accept.suggestions = TRUE)

```

```{r}
L <- mapAgeEnsembleToPaleoData(L)

ageEnsemble <- selectData(L,"ageEnsemble")

```




```{r}
TRSH <- as.lipdTsTibble(L) %>%
  dplyr::filter(paleoData_ecologicalgroup == "TRSH") %>%
  tabulateTs(time.var = "age") %>% 
  rowwise() %>%
  mutate(total = sum(c_across(-age),na.rm = TRUE))
```


```{r}
total <- as.lipdTsTibble(L) %>%
  dplyr::filter(paleoData_ecologicalgroup %in% c("TRSH","UPHE","VACR","SUCC","MANG")) %>%
  tabulateTs(time.var = "age") %>%
  rowwise() %>%
  mutate(total = sum(c_across(-age),na.rm = TRUE))
```


```{r}
treeShrubPercent <- TRSH$total/total$total
plotTimeseriesEnsRibbons(X = ageEnsemble, Y =  treeShrubPercent)

```


```{r}
treeShrubMeanShift <- actR::detectShift(time = ageEnsemble$values,
                  time.variable.name = "age",
                  time.units = "BP",
                  vals = treeShrubPercent,
                  vals.variable.name = "Tree/Shrub",
                  vals.units = "%",
                  summary.bin.step = 500,
                  time.range = c(20000,90000))
```


```{r}
MD03_2707 <- readLipd("https://lipdverse.org/Temp12k/1_0_2/MD03_2707.Weldeab.2007.ensembles.lpd")

mapLipd(MD03_2707,map.type = "stamen",extend.range = 5)
MD03_2707_ts <- mapAgeEnsembleToPaleoData(MD03_2707) %>% as.lipdTsTibble()
```

```{r,warning=FALSE}
MD03_2707_ms <-  actR::detectShift(MD03_2707_ts,
                   vals.variable.name = "SST_from_d18o_ruber_pink_ensemble",
                  summary.bin.step = 500,
                  time.range = c(20000,90000))
```


```{r}
plot(MD03_2707_ms)
```

Selected best practices for organizing and formatting your notebooks are included below.

## The 10 rules
These recommendations are based on (@rule_ten_2019):

1. Tell a story for an audience: interleave explanatory text with code and results to create a computational narrative.

2. Document the process, not just the results: so that others, and yourself later, will understand your reasoning and choices.

3. Use cell divisions to make steps clear: have each piece of R perform one meaningful and documented step, avoid long pieces of R.

4. Modularize code: use and document functions for repeated operations to make the code more readable and save space.

5. Record dependencies: use (for example) Conda package manager, with environment.yml (Conda). Please indicate package versions explicitly. Ideally, include a listing of dependencies at the bottom of the notebook. Make sure you test your notebook in an environment created from these dependencies, so that you don't add undocumented dependencies (we discuss this more in using Binder.

6. Use version control such as git.

7. Build a pipeline: place key variable declarations at the top. Test with different parameters, clean up, and run all R code  to prepare for potential non-interactive execution.

8. Share and explain your data: if the original data are too big, include a sample. Make sure that the data is accessible. Include data description, and any processing done beforehand. Ideally, point to datasets that have been permanently managed and identified by DOIs.

9. Design your notebooks to be read, run, and explored:  
    9.1. Read: store it in a public code repository with a clear README file and a liberal open source license. Generate HTML/PDF versions of the final notebook.  
    9.2. Run: use Binder to run it in the cloud.  
    9.3. Explore: use interactive widgets (e.g., Shiny) 
    
10. Advocate for open research: ask your colleagues or friends to try to run your notebooks.


# References